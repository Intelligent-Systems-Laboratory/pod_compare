{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference Script\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# Detectron imports|\n",
    "from detectron2.engine import launch\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader, MetadataCatalog\n",
    "\n",
    "# Project imports\n",
    "import core.datasets.metadata as metadata\n",
    "\n",
    "from core.setup import setup_config, setup_arg_parser\n",
    "from offline_evaluation import compute_average_precision, compute_probabilistic_metrics, compute_calibration_errors\n",
    "from probabilistic_inference.probabilistic_inference import build_predictor\n",
    "from probabilistic_inference.inference_utils import instances_to_json\n",
    "\n",
    "from baal.active import FileDataset, ActiveLearningDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer, launch\n",
    "from detectron2.evaluation import COCOEvaluator, DatasetEvaluators, verify_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "##setup inference args, this also contains all the training args\n",
    "arg_parser = setup_arg_parser()\n",
    "args = arg_parser.parse_args(\"\")\n",
    "# Support single gpu inference only.\n",
    "args.num_gpus = 1\n",
    "args.dataset_dir = '/public-dataset/BDD/bdd100k'\n",
    "args.test_dataset = 'bdd_val'\n",
    "args.config_file = '/home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout.yaml'\n",
    "args.inference_config = '/home/jefferson.keh/AL_CVPR/src/configs/Inference/bayes_od_mc_dropout.yaml'\n",
    "print(\"Command Line Args:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = setup_config(args, random_seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import get_detection_dataset_dicts\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_detection_dataset_dicts(        \n",
    "    cfg.DATASETS.TRAIN,\n",
    "    filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "    min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "    if cfg.MODEL.KEYPOINT_ON\n",
    "    else 0,\n",
    "    proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(len(dataset), 2000, replace=False)\n",
    "subsample_dataset = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ActiveLearningDataset(dataset)\n",
    "subsample_dataset = ActiveLearningDataset(subsample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.pool.__len__())\n",
    "print(subsample_dataset.pool.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.pool[40764]['file_name'])\n",
    "print(subsample_dataset.pool[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subsample_dataset.__len__())\n",
    "print(subsample_dataset.pool.__len__())\n",
    "print(subsample_dataset.pool[0]['file_name'])\n",
    "subsample_dataset.label(0)\n",
    "print(subsample_dataset.__len__())\n",
    "print(subsample_dataset.pool.__len__())\n",
    "print(subsample_dataset.pool[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "mapper = DatasetMapper(cfg, False)\n",
    "data_loader = build_detection_test_loader(dataset=subsample_dataset.pool, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(data_loader_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-yorkshire",
   "metadata": {},
   "source": [
    "# Other stuff I was just testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data.samplers import TrainingSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader1 = build_detection_train_loader(cfg)\n",
    "\n",
    "sampler2 = TrainingSampler(size=len(dataloader1.dataset.dataset), seed=1234)\n",
    "\n",
    "dataloader2 = build_detection_train_loader(cfg, sampler=sampler2)\n",
    "\n",
    "sampler3 = TrainingSampler(size=4, seed=1234)\n",
    "\n",
    "dataloader3 = build_detection_train_loader(cfg, sampler=sampler3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show that each is a different object\n",
    "print(\"DataLoader 1 Memory Address: \")\n",
    "print(dataloader1)\n",
    "print(\"DataLoader 2 Memory Address: \")\n",
    "print(dataloader2)\n",
    "print(\"DataLoader 3 Memory Address: \")\n",
    "print(dataloader3)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Dataset Lengths\")\n",
    "print(\"DataLoader 1: \", len(dataloader1.dataset.dataset))\n",
    "print(\"DataLoader 2: \", len(dataloader2.dataset.dataset))\n",
    "print(\"DataLoader 3: \", len(dataloader3.dataset.dataset))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sampler Random Seeds\")\n",
    "print(\"DataLoader 1: \", dataloader1.dataset.sampler._seed)\n",
    "print(\"DataLoader 2: \", dataloader2.dataset.sampler._seed)\n",
    "print(\"DataLoader 3: \", dataloader3.dataset.sampler._seed)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sampler Size\")\n",
    "print(\"DataLoader 1: \", dataloader1.dataset.sampler._size)\n",
    "print(\"DataLoader 2: \", dataloader2.dataset.sampler._size)\n",
    "print(\"DataLoader 3: \", dataloader3.dataset.sampler._size)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.initial_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataloader1.dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader2.dataset.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hispanic-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inference Script\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# Detectron imports|\n",
    "from detectron2.engine import launch\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader, MetadataCatalog\n",
    "\n",
    "# Project imports\n",
    "import core.datasets.metadata as metadata\n",
    "\n",
    "from core.setup import setup_config, setup_arg_parser\n",
    "from offline_evaluation import compute_average_precision, compute_probabilistic_metrics, compute_calibration_errors\n",
    "from probabilistic_inference.probabilistic_inference import build_predictor\n",
    "from probabilistic_inference.inference_utils import instances_to_json\n",
    "\n",
    "from baal.active import FileDataset, ActiveLearningDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer, launch\n",
    "from detectron2.evaluation import COCOEvaluator, DatasetEvaluators, verify_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expensive-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Line Args: Namespace(config_file='/home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout.yaml', dataset_dir='/public-dataset/BDD/bdd100k', dist_url='tcp://127.0.0.1:50153', eval_only=False, inference_config='/home/jefferson.keh/AL_CVPR/src/configs/Inference/bayes_od_mc_dropout.yaml', iou_correct=0.7, iou_min=0.1, machine_rank=0, min_allowed_score=0.0, num_gpus=1, num_machines=1, opts=[], random_seed=0, resume=False, test_dataset='bdd_val')\n"
     ]
    }
   ],
   "source": [
    "##setup inference args, this also contains all the training args\n",
    "arg_parser = setup_arg_parser()\n",
    "args = arg_parser.parse_args(\"\")\n",
    "# Support single gpu inference only.\n",
    "args.num_gpus = 1\n",
    "args.dataset_dir = '/public-dataset/BDD/bdd100k'\n",
    "args.test_dataset = 'bdd_val'\n",
    "args.config_file = '/home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout.yaml'\n",
    "args.inference_config = '/home/jefferson.keh/AL_CVPR/src/configs/Inference/bayes_od_mc_dropout.yaml'\n",
    "print(\"Command Line Args:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/../../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "Config '/home/jefferson.keh/AL_CVPR/src/configs/Inference/bayes_od_mc_dropout.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/19 16:49:51 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
      "\u001b[32m[02/19 16:49:52 detectron2]: \u001b[0mEnvironment info:\n",
      "----------------------  ---------------------------------------------------------------------------------------\n",
      "sys.platform            linux\n",
      "Python                  3.8.5 (default, Sep  4 2020, 07:30:14) [GCC 7.3.0]\n",
      "numpy                   1.19.5\n",
      "detectron2              0.3 @/home/jefferson.keh/.conda/envs/pod_jeff/lib/python3.8/site-packages/detectron2\n",
      "Compiler                GCC 7.3\n",
      "CUDA compiler           CUDA 10.2\n",
      "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
      "DETECTRON2_ENV_MODULE   <not set>\n",
      "PyTorch                 1.7.1 @/home/jefferson.keh/.conda/envs/pod_jeff/lib/python3.8/site-packages/torch\n",
      "PyTorch debug build     False\n",
      "GPU available           True\n",
      "GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)\n",
      "CUDA_HOME               /usr/local/cuda\n",
      "Pillow                  8.1.0\n",
      "torchvision             0.8.2 @/home/jefferson.keh/.conda/envs/pod_jeff/lib/python3.8/site-packages/torchvision\n",
      "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
      "fvcore                  0.1.2.post20210115\n",
      "cv2                     4.5.1\n",
      "----------------------  ---------------------------------------------------------------------------------------\n",
      "PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 10.2\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75\n",
      "  - CuDNN 7.6.5\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "\u001b[32m[02/19 16:49:52 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout.yaml', dataset_dir='/public-dataset/BDD/bdd100k', dist_url='tcp://127.0.0.1:50153', eval_only=False, inference_config='/home/jefferson.keh/AL_CVPR/src/configs/Inference/bayes_od_mc_dropout.yaml', iou_correct=0.7, iou_min=0.1, machine_rank=0, min_allowed_score=0.0, num_gpus=1, num_machines=1, opts=[], random_seed=0, resume=False, test_dataset='bdd_val')\n",
      "\u001b[32m[02/19 16:49:52 detectron2]: \u001b[0mContents of args.config_file=/home/jefferson.keh/AL_CVPR/src/configs/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout.yaml:\n",
      "_BASE_: \"Base-BDD-RetinaNet.yaml\"\n",
      "\n",
      "MODEL:\n",
      "    PROBABILISTIC_MODELING:\n",
      "        DROPOUT_RATE: 0.2 # 0.0 for no dropout\n",
      "\n",
      "        # One of the following Loss types: loss_attenuation'.\n",
      "        CLS_VAR_LOSS:\n",
      "            NAME: 'loss_attenuation'\n",
      "            NUM_SAMPLES: 10\n",
      "\n",
      "        # One of the following Loss types: 'none' or 'negative_log_likelihood', 'second_moment_matching', 'energy_loss'.\n",
      "        BBOX_COV_LOSS:\n",
      "            NAME: 'negative_log_likelihood'\n",
      "            COVARIANCE_TYPE: 'diagonal' # One of the following: 'full', 'diagonal' # Settings for dropout\n",
      "\u001b[32m[02/19 16:49:52 detectron2]: \u001b[0mRunning with full config:\n",
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: True\n",
      "  NUM_WORKERS: 8\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ('bdd_val',)\n",
      "  TRAIN: ('bdd_train',)\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (720,)\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_retinanet_resnet_fpn_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: ['res3', 'res4', 'res5']\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: False\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: ProbabilisticRetinaNet\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROBABILISTIC_MODELING:\n",
      "    ANNEALING_STEP: 0\n",
      "    BBOX_COV_LOSS:\n",
      "      COVARIANCE_TYPE: diagonal\n",
      "      NAME: negative_log_likelihood\n",
      "      NUM_SAMPLES: 1000\n",
      "    CLS_VAR_LOSS:\n",
      "      NAME: loss_attenuation\n",
      "      NUM_SAMPLES: 10\n",
      "    DROPOUT_RATE: 0.2\n",
      "    MC_DROPOUT:\n",
      "      \n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res3', 'res4', 'res5']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 7\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.0\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    DROPOUT_RATE: 0.2\n",
      "    FC_DIM: 1024\n",
      "    NAME: \n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 0\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 512\n",
      "    IN_FEATURES: ['res4']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: Res5ROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 80\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
      "    NUM_KEYPOINTS: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['res4']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    POST_NMS_TOPK_TRAIN: 2000\n",
      "    PRE_NMS_TOPK_TEST: 6000\n",
      "    PRE_NMS_TOPK_TRAIN: 12000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl\n",
      "OUTPUT_DIR: /home/jefferson.keh/AL_CVPR/data/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout/random_seed_1234\n",
      "PROBABILISTIC_INFERENCE:\n",
      "  AFFINITY_THRESHOLD: 0.9\n",
      "  BAYES_OD:\n",
      "    BOX_MERGE_MODE: bayesian_inference\n",
      "    CLS_MERGE_MODE: max_score\n",
      "    DIRCH_PRIOR: uniform\n",
      "  ENSEMBLES:\n",
      "    BOX_MERGE_MODE: pre_nms\n",
      "    RANDOM_SEED_NUMS: [0, 1000, 2000, 3000, 4000]\n",
      "  ENSEMBLES_DROPOUT:\n",
      "    BOX_MERGE_MODE: pre_nms\n",
      "  INFERENCE_MODE: bayes_od\n",
      "  MC_DROPOUT:\n",
      "    ENABLE: True\n",
      "    NUM_RUNS: 10\n",
      "SEED: 1234\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BASE_LR: 0.0025\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 30000\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 4\n",
      "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
      "  MAX_ITER: 90000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  STEPS: (60000, 80000)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 1000\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0.0001\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "  EVAL_PERIOD: 0\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: []\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/19 16:49:52 detectron2]: \u001b[0mFull config saved to /home/jefferson.keh/AL_CVPR/data/BDD-Detection/retinanet/retinanet_R_50_FPN_1x_reg_cls_var_dropout/random_seed_1234/config.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = setup_config(args, random_seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entire-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import get_detection_dataset_dicts\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/19 16:49:56 d2.data.datasets.coco]: \u001b[0mLoading /public-dataset/BDD/bdd100k/labels/train_coco_format.json takes 4.31 seconds.\n",
      "\u001b[32m[02/19 16:49:57 d2.data.datasets.coco]: \u001b[0mLoaded 69863 images in COCO format from /public-dataset/BDD/bdd100k/labels/train_coco_format.json\n",
      "\u001b[32m[02/19 16:50:00 d2.data.build]: \u001b[0mRemoved 458 images with no usable annotations. 69405 images left.\n",
      "\u001b[32m[02/19 16:50:01 d2.data.build]: \u001b[0mDistribution of instances among all 7 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|    car     | 713211       |    bus     | 11672        |   truck    | 29971        |\n",
      "|   person   | 91349        |   rider    | 4517         |    bike    | 7210         |\n",
      "|   motor    | 3002         |            |              |            |              |\n",
      "|   total    | 860932       |            |              |            |              |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = get_detection_dataset_dicts(        \n",
    "    cfg.DATASETS.TRAIN,\n",
    "    filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "    min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "    if cfg.MODEL.KEYPOINT_ON\n",
    "    else 0,\n",
    "    proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funny-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(len(dataset), 2000, replace=False)\n",
    "subsample_dataset = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40764 47959 36938 ... 16084 65627 27717]\n"
     ]
    }
   ],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "similar-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ActiveLearningDataset(dataset)\n",
    "subsample_dataset = ActiveLearningDataset(subsample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nearby-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69405\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(dataset.pool.__len__())\n",
    "print(subsample_dataset.pool.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "considerable-candidate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/public-dataset/BDD/bdd100k/images/100k/train/68eae8d4-6c437d9d.jpg\n",
      "/public-dataset/BDD/bdd100k/images/100k/train/68eae8d4-6c437d9d.jpg\n"
     ]
    }
   ],
   "source": [
    "print(dataset.pool[40764]['file_name'])\n",
    "print(subsample_dataset.pool[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "explicit-rainbow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "/public-dataset/BDD/bdd100k/images/100k/train/68eae8d4-6c437d9d.jpg\n",
      "1\n",
      "1999\n",
      "/public-dataset/BDD/bdd100k/images/100k/train/7b583008-124b6809.jpg\n"
     ]
    }
   ],
   "source": [
    "print(subsample_dataset.__len__())\n",
    "print(subsample_dataset.pool.__len__())\n",
    "print(subsample_dataset.pool[0]['file_name'])\n",
    "subsample_dataset.label(0)\n",
    "print(subsample_dataset.__len__())\n",
    "print(subsample_dataset.pool.__len__())\n",
    "print(subsample_dataset.pool[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spread-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/19 16:50:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(720,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/19 16:50:01 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = build_detection_train_loader(cfg, dataset=subsample_dataset.pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "considered-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_iter = iter(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quick-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': '/public-dataset/BDD/bdd100k/images/100k/train/05296fa8-248ff695.jpg', 'height': 720, 'width': 1280, 'image_id': 1935, 'image': tensor([[[ 1,  1,  1,  ...,  2,  2,  2],\n",
      "         [ 1,  1,  1,  ...,  2,  2,  2],\n",
      "         [ 1,  1,  1,  ...,  2,  2,  2],\n",
      "         ...,\n",
      "         [ 2,  2,  2,  ...,  5,  5,  5],\n",
      "         [ 2,  2,  2,  ...,  5,  5,  5],\n",
      "         [ 2,  2,  2,  ...,  5,  5,  5]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  5,  5,  5],\n",
      "         [ 1,  1,  1,  ...,  5,  5,  5],\n",
      "         [ 1,  1,  1,  ...,  5,  5,  5],\n",
      "         ...,\n",
      "         [12, 12, 12,  ..., 16, 16, 16],\n",
      "         [12, 12, 12,  ..., 16, 16, 16],\n",
      "         [12, 12, 12,  ..., 16, 16, 16]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  9,  9,  9],\n",
      "         [ 1,  1,  1,  ...,  9,  9,  9],\n",
      "         [ 1,  1,  1,  ...,  9,  9,  9],\n",
      "         ...,\n",
      "         [29, 29, 29,  ..., 24, 24, 24],\n",
      "         [29, 29, 29,  ..., 24, 24, 24],\n",
      "         [29, 29, 29,  ..., 24, 24, 24]]], dtype=torch.uint8), 'instances': Instances(num_instances=10, image_height=720, image_width=1280, fields=[gt_boxes: Boxes(tensor([[ 356.6620,  429.2274,  556.3027,  560.2416],\n",
      "        [ 555.0549,  435.4662,  658.6185,  506.5881],\n",
      "        [ 648.6365,  437.9617,  678.5826,  489.1196],\n",
      "        [ 658.6185,  437.9617,  696.0511,  476.6420],\n",
      "        [ 688.5646,  440.4572,  718.5107,  470.4033],\n",
      "        [1217.6123,  422.9886, 1277.5045,  521.5612],\n",
      "        [1153.9768,  454.1825, 1220.1078,  506.5881],\n",
      "        [ 753.4478,  446.6960,  765.9254,  456.6780],\n",
      "        [ 774.6596,  441.7049,  792.1282,  455.4302],\n",
      "        [ 850.7726,  417.9976,  919.3990,  451.6870]])), gt_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])}, {'file_name': '/public-dataset/BDD/bdd100k/images/100k/train/1d3ea1ee-1422b357.jpg', 'height': 720, 'width': 1280, 'image_id': 11341, 'image': tensor([[[237, 237, 237,  ...,  46,  47,  47],\n",
      "         [237, 237, 237,  ...,  46,  47,  47],\n",
      "         [237, 237, 237,  ...,  46,  47,  47],\n",
      "         ...,\n",
      "         [ 35,  35,  35,  ..., 115, 115, 115],\n",
      "         [ 35,  35,  35,  ..., 116, 116, 116],\n",
      "         [ 35,  35,  35,  ..., 116, 116, 116]],\n",
      "\n",
      "        [[111, 111, 111,  ...,  41,  42,  42],\n",
      "         [111, 111, 111,  ...,  41,  42,  42],\n",
      "         [111, 111, 111,  ...,  41,  42,  42],\n",
      "         ...,\n",
      "         [ 23,  23,  23,  ...,  83,  83,  83],\n",
      "         [ 23,  23,  23,  ...,  84,  84,  84],\n",
      "         [ 23,  23,  23,  ...,  84,  84,  84]],\n",
      "\n",
      "        [[  0,   0,   0,  ...,  38,  39,  39],\n",
      "         [  0,   0,   0,  ...,  38,  39,  39],\n",
      "         [  0,   0,   0,  ...,  38,  39,  39],\n",
      "         ...,\n",
      "         [ 13,  13,  13,  ...,  64,  64,  64],\n",
      "         [ 13,  13,  13,  ...,  65,  65,  65],\n",
      "         [ 13,  13,  13,  ...,  65,  65,  65]]], dtype=torch.uint8), 'instances': Instances(num_instances=6, image_height=720, image_width=1280, fields=[gt_boxes: Boxes(tensor([[667.3528, 303.2042, 684.8214, 319.4250],\n",
      "        [733.4838, 301.9565, 754.6956, 318.1773],\n",
      "        [723.5016, 301.9565, 733.4837, 311.9385],\n",
      "        [713.5197, 293.2222, 728.4927, 301.9565],\n",
      "        [682.3258, 290.7267, 696.0511, 298.2132],\n",
      "        [709.7764, 280.7447, 719.7584, 289.4789]])), gt_classes: tensor([0, 0, 0, 0, 0, 0])])}, {'file_name': '/public-dataset/BDD/bdd100k/images/100k/train/09edad8a-54347331.jpg', 'height': 720, 'width': 1280, 'image_id': 3872, 'image': tensor([[[27, 29, 31,  ..., 17, 18, 18],\n",
      "         [27, 29, 31,  ..., 17, 17, 18],\n",
      "         [27, 29, 31,  ..., 17, 17, 17],\n",
      "         ...,\n",
      "         [17, 17, 17,  ...,  6,  5,  5],\n",
      "         [16, 17, 17,  ...,  5,  4,  4],\n",
      "         [16, 16, 17,  ...,  4,  4,  4]],\n",
      "\n",
      "        [[37, 39, 41,  ..., 16, 17, 17],\n",
      "         [37, 39, 41,  ..., 16, 16, 17],\n",
      "         [37, 39, 41,  ..., 16, 16, 16],\n",
      "         ...,\n",
      "         [18, 18, 18,  ..., 13, 12, 12],\n",
      "         [17, 18, 18,  ..., 12, 11, 11],\n",
      "         [17, 17, 18,  ..., 11, 11, 11]],\n",
      "\n",
      "        [[21, 23, 25,  ..., 12, 13, 13],\n",
      "         [21, 23, 25,  ..., 12, 12, 13],\n",
      "         [21, 23, 25,  ..., 12, 12, 12],\n",
      "         ...,\n",
      "         [ 8,  8,  8,  ..., 10,  9,  9],\n",
      "         [ 7,  8,  8,  ...,  9,  8,  8],\n",
      "         [ 7,  7,  8,  ...,  8,  8,  8]]], dtype=torch.uint8), 'instances': Instances(num_instances=3, image_height=720, image_width=1280, fields=[gt_boxes: Boxes(tensor([[781.1943, 374.3503, 802.0621, 394.1198],\n",
      "        [716.3943, 369.9571, 766.9164, 399.6114],\n",
      "        [ 72.7877, 352.3842, 206.7809, 417.1842]])), gt_classes: tensor([0, 0, 0])])}, {'file_name': '/public-dataset/BDD/bdd100k/images/100k/train/7728e93c-eede8611.jpg', 'height': 720, 'width': 1280, 'image_id': 46629, 'image': tensor([[[246, 246, 246,  ...,  50,  49,  49],\n",
      "         [246, 246, 246,  ...,  48,  47,  47],\n",
      "         [246, 246, 246,  ...,  45,  44,  44],\n",
      "         ...,\n",
      "         [ 29,  29,  29,  ...,  28,  28,  28],\n",
      "         [ 29,  29,  29,  ...,  28,  28,  28],\n",
      "         [ 29,  29,  29,  ...,  28,  28,  28]],\n",
      "\n",
      "        [[213, 213, 213,  ...,  56,  55,  55],\n",
      "         [213, 213, 213,  ...,  54,  53,  53],\n",
      "         [213, 213, 213,  ...,  51,  50,  50],\n",
      "         ...,\n",
      "         [ 26,  26,  26,  ...,  27,  27,  27],\n",
      "         [ 26,  26,  26,  ...,  27,  27,  27],\n",
      "         [ 26,  26,  26,  ...,  27,  27,  27]],\n",
      "\n",
      "        [[168, 168, 168,  ...,  45,  44,  44],\n",
      "         [168, 168, 168,  ...,  43,  42,  42],\n",
      "         [168, 168, 168,  ...,  40,  39,  39],\n",
      "         ...,\n",
      "         [ 22,  22,  22,  ...,  23,  23,  23],\n",
      "         [ 22,  22,  22,  ...,  23,  23,  23],\n",
      "         [ 22,  22,  22,  ...,  23,  23,  23]]], dtype=torch.uint8), 'instances': Instances(num_instances=11, image_height=720, image_width=1280, fields=[gt_boxes: Boxes(tensor([[ 857.2070,  396.7858, 1131.7129,  520.3135],\n",
      "        [ 757.3867,  409.2633,  904.6217,  520.3135],\n",
      "        [ 656.3186,  425.4841,  746.1569,  487.8718],\n",
      "        [ 546.5163,  434.2184,  565.2326,  449.1915],\n",
      "        [ 646.3366,  432.9706,  672.5394,  477.8898],\n",
      "        [ 626.3726,  434.2184,  657.5664,  474.1465],\n",
      "        [ 606.4084,  431.7229,  635.1068,  470.4033],\n",
      "        [ 595.1786,  435.4662,  617.6382,  465.4123],\n",
      "        [ 586.4444,  432.9706,  603.9130,  460.4212],\n",
      "        [ 578.9579,  432.9706,  596.4265,  461.6690],\n",
      "        [ 566.4803,  435.4662,  581.4534,  449.1914]])), gt_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])}]\n"
     ]
    }
   ],
   "source": [
    "print(next(train_data_loader_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interim-testimony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/19 16:50:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "mapper = DatasetMapper(cfg, False)\n",
    "test_data_loader = build_detection_test_loader(dataset=subsample_dataset.pool, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "german-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader_iter = iter(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ahead-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': '/public-dataset/BDD/bdd100k/images/100k/train/7b583008-124b6809.jpg', 'height': 720, 'width': 1280, 'image_id': 48271, 'image': tensor([[[189, 189, 189,  ..., 169, 169, 169],\n",
      "         [189, 189, 189,  ..., 169, 169, 169],\n",
      "         [189, 189, 189,  ..., 169, 169, 169],\n",
      "         ...,\n",
      "         [201, 201, 201,  ...,  65,  65,  65],\n",
      "         [201, 201, 201,  ...,  64,  64,  64],\n",
      "         [201, 201, 201,  ...,  64,  64,  64]],\n",
      "\n",
      "        [[147, 147, 147,  ..., 130, 130, 130],\n",
      "         [147, 147, 147,  ..., 130, 130, 130],\n",
      "         [147, 147, 147,  ..., 130, 130, 130],\n",
      "         ...,\n",
      "         [219, 219, 219,  ...,  32,  32,  32],\n",
      "         [219, 219, 219,  ...,  31,  31,  31],\n",
      "         [219, 219, 219,  ...,  31,  31,  31]],\n",
      "\n",
      "        [[110, 110, 110,  ...,  98,  98,  98],\n",
      "         [110, 110, 110,  ...,  98,  98,  98],\n",
      "         [110, 110, 110,  ...,  98,  98,  98],\n",
      "         ...,\n",
      "         [206, 206, 206,  ...,  36,  36,  36],\n",
      "         [206, 206, 206,  ...,  35,  35,  35],\n",
      "         [206, 206, 206,  ...,  35,  35,  35]]], dtype=torch.uint8)}]\n"
     ]
    }
   ],
   "source": [
    "print(next(test_data_loader_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-yorkshire",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data.samplers import TrainingSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader1 = build_detection_train_loader(cfg)\n",
    "\n",
    "sampler2 = TrainingSampler(size=len(dataloader1.dataset.dataset), seed=1234)\n",
    "\n",
    "dataloader2 = build_detection_train_loader(cfg, sampler=sampler2)\n",
    "\n",
    "sampler3 = TrainingSampler(size=4, seed=1234)\n",
    "\n",
    "dataloader3 = build_detection_train_loader(cfg, sampler=sampler3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show that each is a different object\n",
    "print(\"DataLoader 1 Memory Address: \")\n",
    "print(dataloader1)\n",
    "print(\"DataLoader 2 Memory Address: \")\n",
    "print(dataloader2)\n",
    "print(\"DataLoader 3 Memory Address: \")\n",
    "print(dataloader3)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Dataset Lengths\")\n",
    "print(\"DataLoader 1: \", len(dataloader1.dataset.dataset))\n",
    "print(\"DataLoader 2: \", len(dataloader2.dataset.dataset))\n",
    "print(\"DataLoader 3: \", len(dataloader3.dataset.dataset))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sampler Random Seeds\")\n",
    "print(\"DataLoader 1: \", dataloader1.dataset.sampler._seed)\n",
    "print(\"DataLoader 2: \", dataloader2.dataset.sampler._seed)\n",
    "print(\"DataLoader 3: \", dataloader3.dataset.sampler._seed)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sampler Size\")\n",
    "print(\"DataLoader 1: \", dataloader1.dataset.sampler._size)\n",
    "print(\"DataLoader 2: \", dataloader2.dataset.sampler._size)\n",
    "print(\"DataLoader 3: \", dataloader3.dataset.sampler._size)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.initial_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataloader1.dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader2.dataset.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
